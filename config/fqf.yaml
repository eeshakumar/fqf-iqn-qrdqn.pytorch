#num_steps: 50000000  # It equals to few frames, run locally.
#batch_size: 32
#N: 32
#num_cosines: 64
#ent_coef: 0  # You can use entropy loss as a regularizer.
#kappa: 1.0
#quantile_lr: 5.e-5
#fraction_lr: 2.5e-9
#memory_size: 100
#gamma: 0.99
#multi_step: 1
#update_interval: 4
#target_update_interval: 10
#start_steps: 50
#epsilon_train: 0.01
#epsilon_eval: 0.001
#epsilon_decay_steps: 250  # It equals to 1M frames.
#double_q_learning: False
#dueling_net: False
#noisy_net: False
#use_per: False
#log_interval: 100
#eval_interval: 250
#num_eval_steps: 125  # It equals to 500k frames.
#max_episode_steps: 27
#grad_cliping:
num_steps: 500000  # It equals to 200M frames.
batch_size: 1024
N: 40
num_cosines: 64
ent_coef: 0  # You can use entropy loss as a regularizer.
kappa: 1.0
quantile_lr: 5.e-5
fraction_lr: 2.5e-9
memory_size: 1000
gamma: 0.99
multi_step: 1
update_interval: 1
target_update_interval: 1
start_steps: 500
epsilon_train: 0.01
epsilon_eval: 0.001
epsilon_decay_steps: 250  # It equals to 1M frames.
double_q_learning: True
dueling_net: True
noisy_net: False
use_per: True
log_interval: 100
eval_interval: 250
num_eval_steps: 125  # It equals to 500k frames.
max_episode_steps: 27 # from common_params
grad_cliping: True